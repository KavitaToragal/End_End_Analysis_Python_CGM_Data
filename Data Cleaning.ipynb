{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0e7ec3-7b3d-4874-a7b4-1efafdfc7c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¼ Cleaning HUPA0001P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0001P.csv\n",
      "ðŸ§¼ Cleaning HUPA0002P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0002P.csv\n",
      "ðŸ§¼ Cleaning HUPA0003P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0003P.csv\n",
      "ðŸ§¼ Cleaning HUPA0004P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0004P.csv\n",
      "ðŸ§¼ Cleaning HUPA0005P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0005P.csv\n",
      "ðŸ§¼ Cleaning HUPA0006P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0006P.csv\n",
      "ðŸ§¼ Cleaning HUPA0007P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0007P.csv\n",
      "ðŸ§¼ Cleaning HUPA0009P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0009P.csv\n",
      "ðŸ§¼ Cleaning HUPA0010P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0010P.csv\n",
      "ðŸ§¼ Cleaning HUPA0011P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0011P.csv\n",
      "ðŸ§¼ Cleaning HUPA0014P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0014P.csv\n",
      "ðŸ§¼ Cleaning HUPA0015P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0015P.csv\n",
      "ðŸ§¼ Cleaning HUPA0016P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0016P.csv\n",
      "ðŸ§¼ Cleaning HUPA0017P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0017P.csv\n",
      "ðŸ§¼ Cleaning HUPA0018P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0018P.csv\n",
      "ðŸ§¼ Cleaning HUPA0019P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0019P.csv\n",
      "ðŸ§¼ Cleaning HUPA0020P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0020P.csv\n",
      "ðŸ§¼ Cleaning HUPA0021P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0021P.csv\n",
      "ðŸ§¼ Cleaning HUPA0022P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0022P.csv\n",
      "ðŸ§¼ Cleaning HUPA0023P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0023P.csv\n",
      "ðŸ§¼ Cleaning HUPA0024P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0024P.csv\n",
      "ðŸ§¼ Cleaning HUPA0025P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0025P.csv\n",
      "ðŸ§¼ Cleaning HUPA0026P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0026P.csv\n",
      "ðŸ§¼ Cleaning HUPA0027P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0027P.csv\n",
      "ðŸ§¼ Cleaning HUPA0028P.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA0028P.csv\n",
      "ðŸ§¼ Cleaning T1DM_patient_sleep_demographics_with_race.csv\n",
      "âœ… Saved: C:\\PYTHON HACK\\Cleaned_data\\cleaned_T1DM_patient_sleep_demographics_with_race.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing all raw patient files\n",
    "raw_folder = r'C:\\PYTHON HACK'\n",
    "cleaned_folder = r'C:\\PYTHON HACK\\Cleaned_data'\n",
    "os.makedirs(cleaned_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all CSVs\n",
    "for file in os.listdir(raw_folder):\n",
    "    if file.lower().endswith('.csv'):\n",
    "        file_path = os.path.join(raw_folder, file)\n",
    "        print(f\"ðŸ§¼ Cleaning {file}\")\n",
    "\n",
    "        try:\n",
    "            # Load with flexible delimiter handling\n",
    "            df = pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip')\n",
    "\n",
    "            # Standardize column names\n",
    "            df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "            # Parse time column if present\n",
    "            time_cols = [col for col in df.columns if 'time' in col]\n",
    "            if time_cols:\n",
    "                df[time_cols[0]] = pd.to_datetime(df[time_cols[0]], errors='coerce')\n",
    "                df.dropna(subset=[time_cols[0]], inplace=True)\n",
    "\n",
    "            # Drop fully empty rows\n",
    "            df.dropna(how='all', inplace=True)\n",
    "\n",
    "            # Save cleaned file\n",
    "            cleaned_path = os.path.join(cleaned_folder, f'cleaned_{file}')\n",
    "            df.to_csv(cleaned_path, index=False)\n",
    "            print(f\"âœ… Saved: {cleaned_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to clean {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b35e08b-090b-4ca2-bece-e6841c2afc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged file saved at: C:\\PYTHON HACK\\Cleaned_data\\merged_patients.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to cleaned files\n",
    "cleaned_folder = r'C:\\PYTHON HACK\\Cleaned_data'\n",
    "\n",
    "# Grab all patient files (excluding demographics)\n",
    "patient_files = glob.glob(os.path.join(cleaned_folder, 'cleaned_HUPA*.csv'))\n",
    "\n",
    "# Merge all into one DataFrame\n",
    "merged_df = pd.concat([pd.read_csv(f) for f in patient_files], ignore_index=True)\n",
    "\n",
    "# Optional: Add filename as patient ID if not already present\n",
    "merged_df['source_file'] = [os.path.basename(f) for f in patient_files for _ in range(len(pd.read_csv(f)))]\n",
    "\n",
    "# Save merged file\n",
    "merged_path = os.path.join(cleaned_folder, 'merged_patients.csv')\n",
    "merged_df.to_csv(merged_path, index=False)\n",
    "\n",
    "print(f\"âœ… Merged file saved at: {merged_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f28b38-bfeb-429e-b81a-af930d53920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['patient_id'] = merged_df['source_file'].str.extract(r'(HUPA\\d{4}P)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c5d24c-8cdb-4f30-b081-a4a2be7da0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob(r'C:\\PYTHON HACK\\Cleaned_data\\cleaned_HUPA*.csv')\n",
    "merged_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "merged_df.to_csv(r'C:\\PYTHON HACK\\Cleaned_data\\merged_patients.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cce8bb-20fd-4c85-b261-8a0420a024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add source file name to each row\n",
    "merged_df['source_file'] = [os.path.basename(f) for f in all_files for _ in range(len(pd.read_csv(f)))]\n",
    "\n",
    "# Extract patient_id from filename (e.g., HUPA0001P)\n",
    "merged_df['patient_id'] = merged_df['source_file'].str.extract(r'(HUPA\\d{4}P)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ada5d58-f229-4753-9b2e-03e83958e7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched rows: 309392\n",
      "Unmatched rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Matched rows:\", final_df['age'].notna().sum())\n",
    "print(\"Unmatched rows:\", final_df['age'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4c3df0-4f40-437e-bf94-4f6955f7e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Time features added and saved to Excel!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset.csv')\n",
    "\n",
    "# Convert 'time' column to datetime format\n",
    "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "# Create new columns\n",
    "df['date'] = df['time'].dt.date\n",
    "df['clock_time'] = df['time'].dt.time\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['day_of_week'] = df['time'].dt.day_name()\n",
    "\n",
    "# Save the updated dataset to Excel\n",
    "df.to_excel(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset_with_time_features.xlsx', index=False)\n",
    "\n",
    "print(\"âœ… Time features added and saved to Excel!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6b8613-b6be-473b-bd5d-e90d9e4e6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Glucose features added and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset.csv')\n",
    "\n",
    "# Step 1: Fill missing glucose values with the median\n",
    "df['glucose'] = df['glucose'].fillna(df['glucose'].median())\n",
    "\n",
    "# Step 2: Flag glucose levels\n",
    "df['glucose_flag'] = pd.cut(\n",
    "    df['glucose'],\n",
    "    bins=[0, 70, 180, 300],\n",
    "    labels=['Low', 'Normal', 'High'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Step 3: Convert mg/dL to mmol/L (1 mmol/L = 18 mg/dL)\n",
    "df['glucose_mmol'] = df['glucose'] / 18\n",
    "\n",
    "# Optional: Round mmol values for cleaner display\n",
    "df['glucose_mmol'] = df['glucose_mmol'].round(2)\n",
    "\n",
    "# Save updated file\n",
    "df.to_excel(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset_with_glucose.xlsx', index=False)\n",
    "\n",
    "print(\"âœ… Glucose features added and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dad0e8-3d3e-46d9-8eea-b649f351fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize calories to a 0â€“1 scale\n",
    "df['calories_norm'] = (df['calories'] - df['calories'].min()) / (df['calories'].max() - df['calories'].min())\n",
    "\n",
    "# Flag days with high calorie burn\n",
    "df['high_burn'] = df['calories'] > 2500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fa2cc1-7f90-4c55-9852-edf398fdcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rolling average with a window of 5 readings\n",
    "df['hr_rolling'] = df['heart_rate'].rolling(window=5, min_periods=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19d3f70-015c-4c1b-8b78-f11775f24bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Heart rate smoothed and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset.csv')\n",
    "\n",
    "# Step 1: Convert 'time' column to datetime if not already\n",
    "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "# Step 2: Sort by time to ensure rolling window follows chronological order\n",
    "df = df.sort_values('time')\n",
    "\n",
    "# Step 3: Apply rolling average with a window of 5 readings\n",
    "df['hr_rolling'] = df['heart_rate'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "# Optional: Round for cleaner display\n",
    "df['hr_rolling'] = df['hr_rolling'].round(2)\n",
    "\n",
    "# Save the updated file\n",
    "df.to_excel(r'C:\\PYTHON HACK\\Cleaned_data\\final_dataset_with_hr_smooth.xlsx', index=False)\n",
    "\n",
    "print(\"âœ… Heart rate smoothed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24923c2a-ec0d-4860-b435-e3c39681fe3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
